{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding parallel text in the Mambai Language Manual through PDFMiner\n",
    "\n",
    "- Inputs: `Mambai to English dict.docx`, `English to Mambai dict.docx`\n",
    "- Outputs: dictionaries `mgm_eng.json`, `eng_mgm.json`\n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. Setup Python requirements: `python3 -m venv .venv && source .venv/bin/activate && pip install -r requirements.txt`\n",
    "2. Run this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "almeida du santus jame du santus jeamen\n",
      "almeido dos santos jaime dos santos gmn\n",
      "\n",
      "niapreni bua tototu sine mai ubuka no transforma nia sai ema nee be ia valor kadidade\n",
      "nia aprende buat hotuhotu husi nia mai eduka no transforma nia sai ema neebe iha valor karidade\n",
      "\n",
      "loska sala se los siradn los sala deansira bel hadia\n",
      "loos ka sala se loos sira dehan loos sala dehan sira bele hadia\n",
      "\n",
      "ita aberta kole aberta ba sirab hodi nune sira mas bele komprende sira mose bele hala\n",
      "ita aberta koalia aberta ba sira hodi nunee sira mos bele komprende sira mos bele halao\n",
      "\n",
      "ita timor dia klivita tur hatur no nok atu hasilesion ne te inpase politikeda ne hodi hare dezenvolvimentu nasional no hasoru se karik buat ne akontesia ita nerai\n",
      "ita timor diak liu ita tuur ita tuur nonook atu solusiona impasse politika ida nee hodi haree dezenvolvimentu nasional no hasoru se karik buat nee akontese iha itania rai\n",
      "\n",
      "agora fakata dalan ba mi bobu kikina kasian\n",
      "agora taka dalan ba ami povu kiik nee kasihan\n",
      "\n",
      "sasan nebe agora ami uza ne ida neebe mak  sira sames lorio ne mai iha fula kotuk neebe ami uza sito agora\n",
      "sasan neebe agora ami uza nee ida neebe maka sira sames lori ona mai iha fulan kotuk neebe ami uza sei too agora\n",
      "\n",
      "hantanin liki rasi kosar na hakee mu\n",
      "ami paling liki rasik kosar par han hemu\n",
      "\n",
      "diskulfami en komodaitu<pad>\n",
      "deskulpa ami inkomoda uitoan\n",
      "\n",
      "haufiar hane maunbotsananahaufiar sn pursentuhaufiar ma maun bot sa nana serake i haoko se ne bele ga lae<pad>\n",
      "hau fiar haunia maun boot xanana hau fiar cen porsentu hau fiar mak maun boot xanana serake iha okos nee bele ka lae\n",
      "\n",
      "hanesan iha batu lari neba ne emania aman saiusin natar laran maidean covid laimos pulisi baba brikatin seriak covid nebitataunsa<pad>\n",
      "hanesan iha uatolari neeba nee ema nia aman sai husi natar laran mai dehan covid laiha mos polisia ba obriga tenke ser iha covid neebe ita atu halo nuusa\n",
      "\n",
      "ami buko ki pova kik na balin futur<pad>\n",
      "ami povu kiik nee paling futuru\n",
      "\n",
      "ho volumi mota neebe bot hakotu hotu kanu kanalizasaun be mos i hihaparte maloanian<pad>\n",
      "ho volume mota neebe boot hakotu hotu kanu kanalizasaun bee moos iha parte maloha nian\n",
      "\n",
      "importante haturba tutu hotu ho komitmentu diakontade bot atu hala mudansa<pad>\n",
      "importante hatuur buat hotuhotu ho kometimentu no iha vontade boot atu halo mudansa\n",
      "\n",
      "<pad>\n",
      "la iha buat ida\n",
      "\n",
      "maisr nain tolu hanuin katakokt iha si iha mrafat<pad>\n",
      "mais sira nain tolu hanoin katak covid iha sei iha nafatin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds, refs = [\n",
    "    \"almeida du santus jame du santus jeamen\",\n",
    "    \"niapreni bua tototu sine mai ubuka no transforma nia sai ema nee be ia valor kadidade\",\n",
    "    \"loska sala se los siradn los sala deansira bel hadia\",\n",
    "    \"ita aberta kole aberta ba sirab hodi nune sira mas bele komprende sira mose bele hala\",\n",
    "    \"ita timor dia klivita tur hatur no nok atu hasilesion ne te inpase politikeda ne hodi hare dezenvolvimentu nasional no hasoru se karik buat ne akontesia ita nerai\",\n",
    "    \"agora fakata dalan ba mi bobu kikina kasian\",\n",
    "    \"sasan nebe agora ami uza ne ida neebe mak  sira sames lorio ne mai iha fula kotuk neebe ami uza sito agora\",\n",
    "    \"hantanin liki rasi kosar na hakee mu\",\n",
    "    \"diskulfami en komodaitu<pad>\",\n",
    "    \"haufiar hane maunbotsananahaufiar sn pursentuhaufiar ma maun bot sa nana serake i haoko se ne bele ga lae<pad>\",\n",
    "    \"hanesan iha batu lari neba ne emania aman saiusin natar laran maidean covid laimos pulisi baba brikatin seriak covid nebitataunsa<pad>\",\n",
    "    \"ami buko ki pova kik na balin futur<pad>\",\n",
    "    \"ho volumi mota neebe bot hakotu hotu kanu kanalizasaun be mos i hihaparte maloanian<pad>\",\n",
    "    \"importante haturba tutu hotu ho komitmentu diakontade bot atu hala mudansa<pad>\",\n",
    "    \"<pad>\",\n",
    "    \"maisr nain tolu hanuin katakokt iha si iha mrafat<pad>\",\n",
    "], [\n",
    "    \"almeido dos santos jaime dos santos gmn\",\n",
    "    \"nia aprende buat hotuhotu husi nia mai eduka no transforma nia sai ema neebe iha valor karidade\",\n",
    "    \"loos ka sala se loos sira dehan loos sala dehan sira bele hadia\",\n",
    "    \"ita aberta koalia aberta ba sira hodi nunee sira mos bele komprende sira mos bele halao\",\n",
    "    \"ita timor diak liu ita tuur ita tuur nonook atu solusiona impasse politika ida nee hodi haree dezenvolvimentu nasional no hasoru se karik buat nee akontese iha itania rai\",\n",
    "    \"agora taka dalan ba ami povu kiik nee kasihan\",\n",
    "    \"sasan neebe agora ami uza nee ida neebe maka sira sames lori ona mai iha fulan kotuk neebe ami uza sei too agora\",\n",
    "    \"ami paling liki rasik kosar par han hemu\",\n",
    "    \"deskulpa ami inkomoda uitoan\",\n",
    "    \"hau fiar haunia maun boot xanana hau fiar cen porsentu hau fiar mak maun boot xanana serake iha okos nee bele ka lae\",\n",
    "    \"hanesan iha uatolari neeba nee ema nia aman sai husi natar laran mai dehan covid laiha mos polisia ba obriga tenke ser iha covid neebe ita atu halo nuusa\",\n",
    "    \"ami povu kiik nee paling futuru\",\n",
    "    \"ho volume mota neebe boot hakotu hotu kanu kanalizasaun bee moos iha parte maloha nian\",\n",
    "    \"importante hatuur buat hotuhotu ho kometimentu no iha vontade boot atu halo mudansa\",\n",
    "    \"la iha buat ida\",\n",
    "    \"mais sira nain tolu hanoin katak covid iha sei iha nafatin\",\n",
    "]\n",
    "\n",
    "for pred, ref in zip(preds, refs):\n",
    "    print(pred)\n",
    "    print(ref)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DictionaryEntry:\n",
    "    entry: str\n",
    "    definition: str\n",
    "    part_of_speech: str = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Dictionary:\n",
    "    entries: list[DictionaryEntry]\n",
    "\n",
    "    def save_as_json(self, path):\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump([entry.__dict__ for entry in self.entries], f, indent=2)\n",
    "\n",
    "    def load_from_json(self, path):\n",
    "        with open(path, \"r\") as f:\n",
    "            entries = json.load(f)\n",
    "            self.entries = [DictionaryEntry(**entry) for entry in entries]\n",
    "\n",
    "\n",
    "def parse_key(key: str):\n",
    "    keys = [k.strip() for k in key.split(\", \")]\n",
    "    return keys\n",
    "\n",
    "\n",
    "def parse_value(value: str):\n",
    "    \"\"\"Parse a raw value into a list of definitions, each with a part of speech.\"\"\"\n",
    "    value = value.replace(\"\\n\", \" \")\n",
    "    # split to get multiple definitions\n",
    "    values = re.split(r\";|,\", value)\n",
    "    for value in values:\n",
    "        # extract part of speech from the value, if present: max 3 letters, ends with a '.', e.g. \"adj.\" or \"n.\"\n",
    "        value = value.strip(\". \")\n",
    "        match = re.match(r\"([a-z]{1,3}\\.)(.+)$\", value)\n",
    "        if match:\n",
    "            part_of_speech = match.group(1).strip()\n",
    "            value = match.group(2).strip()\n",
    "            yield {\"part_of_speech\": part_of_speech, \"value\": value}\n",
    "        else:\n",
    "            yield {\"part_of_speech\": None, \"value\": value}\n",
    "\n",
    "\n",
    "assert list(parse_value(\"n. person\")) == [{\"part_of_speech\": \"n.\", \"value\": \"person\"}]\n",
    "assert list(parse_value(\"n. (artificial) light.\")) == [\n",
    "    {\"part_of_speech\": \"n.\", \"value\": \"(artificial) light\"}\n",
    "]\n",
    "assert list(parse_value(\"adj. short; shallow.\")) == [\n",
    "    {\"part_of_speech\": \"adj.\", \"value\": \"short\"},\n",
    "    {\"part_of_speech\": None, \"value\": \"shallow\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "\n",
    "def extract_raw_data_from_docx(docx_path) -> dict:\n",
    "    doc = Document(docx_path)\n",
    "    data = {}\n",
    "    current_key = None\n",
    "\n",
    "    for paragraph in doc.paragraphs:\n",
    "        for run in paragraph.runs:\n",
    "            if run.bold:\n",
    "                if current_key:\n",
    "                    # Finalize the previous entry\n",
    "                    data[current_key] = data.get(current_key, \"\").strip()\n",
    "                current_key = run.text\n",
    "                data[current_key] = \"\"\n",
    "            else:\n",
    "                if current_key:\n",
    "                    data[current_key] += run.text\n",
    "\n",
    "    # Finalizing the last entry\n",
    "    if current_key:\n",
    "        data[current_key] = data.get(current_key, \"\").strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_dictionary_from_raw_data(raw_data: dict) -> Dictionary:\n",
    "    dictionary = Dictionary([])\n",
    "\n",
    "    for key, value in raw_data.items():\n",
    "        keys = parse_key(key)\n",
    "        values = list(parse_value(value))\n",
    "        # create a new entry for each key/value combination\n",
    "        for key in keys:\n",
    "            for value in values:\n",
    "                part_of_speech = value[\"part_of_speech\"]\n",
    "                value = value[\"value\"]\n",
    "                if key and value:\n",
    "                    dictionary.entries.append(\n",
    "                        DictionaryEntry(\n",
    "                            entry=key, definition=value, part_of_speech=part_of_speech\n",
    "                        )\n",
    "                    )\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def get_dictionary(path):\n",
    "    raw_data = extract_raw_data_from_docx(file_path)\n",
    "    dictionary = get_dictionary_from_raw_data(raw_data)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/raphaelmerx/Downloads/Mambai/Mambai to English dict.docx\"\n",
    "mgm_eng_dictionary = get_dictionary(file_path)\n",
    "mgm_eng_dictionary.save_as_json(\"mgm_eng.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DictionaryEntry(entry='too', definition='~', part_of_speech=None),\n",
       " DictionaryEntry(entry=\"p√¥s\\nlet's\", definition='ma', part_of_speech=None),\n",
       " DictionaryEntry(entry='indeed', definition='didi', part_of_speech=None),\n",
       " DictionaryEntry(entry='witch', definition='sabu', part_of_speech=None),\n",
       " DictionaryEntry(entry='a ~,', definition='(after a vowel) kene', part_of_speech=None),\n",
       " DictionaryEntry(entry='help', definition='ajuda', part_of_speech=None),\n",
       " DictionaryEntry(entry='blue', definition='moro', part_of_speech=None),\n",
       " DictionaryEntry(entry='bother', definition='dlai', part_of_speech=None),\n",
       " DictionaryEntry(entry='baby', definition='an-koso', part_of_speech=None),\n",
       " DictionaryEntry(entry='feather', definition='man-hulu', part_of_speech=None)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/Users/raphaelmerx/Downloads/Mambai/English to Mambai dict.docx\"\n",
    "eng_mgm_dictionary = get_dictionary(file_path)\n",
    "eng_mgm_dictionary.save_as_json(\"eng_mgm.json\")\n",
    "\n",
    "import random\n",
    "\n",
    "random.sample(eng_mgm_dictionary.entries, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build mgm-eng.stem.dic for hunalign\n",
    "# note that eng is target but format with be \"eng_value @ mgm_key\"\n",
    "\n",
    "with open(\"mgm-eng.stem.dic\", \"w\") as f:\n",
    "    for entry in mgm_eng_dictionary.entries:\n",
    "        f.write(f\"{entry.definition} @ {entry.entry}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
