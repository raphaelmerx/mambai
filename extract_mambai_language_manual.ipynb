{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding parallel text in the Mambai Language Manual\n",
    "\n",
    "Input: `Mambai Language Manual.docx`\n",
    "Output: `mambai_parallel_text.csv`\n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. Setup Python requirements: `python3 -m venv .venv && source .venv/bin/activate && pip install -r requirements.txt`\n",
    "2. Put your OPENAI_API_KEY in `.env`\n",
    "3. Run this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docx2txt\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "\n",
    "# openai api key should be in .env file under OPENAI_API_KEY\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text length: 144322\n",
      "Total words: 25560\n"
     ]
    }
   ],
   "source": [
    "filename = \"Mambai Language Manual.docx\"\n",
    "\n",
    "# get text from this docx using docx2txt\n",
    "text = docx2txt.process(filename)\n",
    "\n",
    "# start at the Grammar section\n",
    "text = text[text.index(\"Mambai has a very simple grammatical structure,\") :]\n",
    "\n",
    "print(f\"Total text length: {len(text)}\")\n",
    "print(f\"Total words: {len(text.split())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 25560\n",
      "Total sections: 103\n",
      "Total words in all sections combined: 50870\n"
     ]
    }
   ],
   "source": [
    "# from the text, create a string of 500 words at a time, each overalapping with the last by 250 words\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "words = text.split()\n",
    "print(f\"Total words: {len(words)}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Section:\n",
    "    text: str\n",
    "    parallel_text: List[str]\n",
    "\n",
    "\n",
    "sliding_window = 500\n",
    "overlap = 250\n",
    "sections = []\n",
    "for i in range(0, len(words), sliding_window - overlap):\n",
    "    text = \" \".join(words[i : i + sliding_window])\n",
    "    section = Section(text=text, parallel_text=[])\n",
    "    sections.append(section)\n",
    "\n",
    "print(f\"Total sections: {len(sections)}\")\n",
    "print(\n",
    "    f'Total words in all sections combined: {len(\" \".join([s.text for s in sections]).split())}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"\"\"You are a research assistant in linguistics, rigorously sorting through text.\"\"\"\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"The following is extracted from a Mambai language manual:\n",
    "\n",
    "START\n",
    "{text}\n",
    "END\n",
    "\n",
    "Give me the parallel Mambai/English words or phrases mentioned in this section, in json format. Ignore other text, and ignore phrases that are not parallel.\n",
    "\n",
    "Example:\n",
    "START\n",
    "The following conjunctions are best introduced in context:\n",
    "Atmen im lao\n",
    "Im lao hal\n",
    "Arpi'l im lao\n",
    "if\n",
    "if you go\n",
    "when you go\n",
    "END\n",
    "\n",
    "Expected result:\n",
    "[\n",
    "  {{\"Mambai\": \"Atmen im lao\", \"English\": \"if\"}},\n",
    "  {{\"Mambai\": \"Im lao hal\", \"English\": \"if you go\"}},\n",
    "  {{\"Mambai\": \"Arpi'l im lao\", \"English\": \"when you go\"}}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "section = sections[0]\n",
    "\n",
    "\n",
    "def get_section_parallel_text(section):\n",
    "    if len(section.parallel_text) > 0:\n",
    "        print(\"Already have parallel text for this section, skipping\")\n",
    "        return\n",
    "    prompt = prompt_template.format(text=section.text)\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        # takes over 3 min per section for both!\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        # model=\"gpt-4\"\n",
    "    )\n",
    "\n",
    "    result = chat_completion.choices[0].message.content\n",
    "    if \"```\" in result:\n",
    "        # start at the first ```, end at the last ```, and strip out the ```s\n",
    "        result = result[result.index(\"```\") :]\n",
    "        result = result[: result.rindex(\"```\")]\n",
    "        result = result[7:]\n",
    "\n",
    "    result = json.loads(result)\n",
    "    section.parallel_text = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "sections_to_process = [s for s in sections if len(s.parallel_text) == 0]\n",
    "\n",
    "# Takes a few hours, depending on OpenAI response times\n",
    "# this is idempotent: if you run it again, it will skip sections that already have parallel text\n",
    "for section in tqdm(sections_to_process):\n",
    "    get_section_parallel_text(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"mambai_sections.pickle\", \"wb\") as f:\n",
    "    pickle.dump(sections, f)\n",
    "\n",
    "# with open(\"mambai_sections.pickle\", \"rb\") as f:\n",
    "#     sections = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 6670 parallel text items\n"
     ]
    }
   ],
   "source": [
    "# get a parallel_text in all sections\n",
    "\n",
    "all_parallel_text = []\n",
    "for section in sections:\n",
    "    all_parallel_text.extend(section.parallel_text)\n",
    "\n",
    "print(f\"Total of {len(all_parallel_text)} parallel text items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 980 lists of strings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Mambai': 'kapé', 'English': 'coffee'},\n",
       " {'Mambai': 'Au hakarak kuartu nor kama kid, nei riu-hati nor sentina.',\n",
       "  'English': 'I want a single room with bath and toilet.'},\n",
       " {'Mambai': 'gme adj', 'English': 'yellow'},\n",
       " {'Mambai': 'mro', 'English': 'thirsty'},\n",
       " {'Mambai': 'gal n', 'English': 'bag'},\n",
       " {'Mambai': 'ble kek', 'English': 'awake'},\n",
       " {'Mambai': 'Rom akuza urá dêssáp?',\n",
       "  'English': 'Have you been accused of something?'},\n",
       " {'Mambai': 'pun-klao', 'English': 'to damage; to harm,hurt'},\n",
       " {'Mambai': 'id', 'English': 'a'},\n",
       " {'Mambai': 'mendai', 'English': 'like this, thus, so'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(all_parallel_text, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 5170 unique parallel text items\n",
      "Total of 5164 unique parallel text items\n"
     ]
    }
   ],
   "source": [
    "# only keep unique items, in a tuple (Mambai, English)\n",
    "unique_parallel_text = []\n",
    "for parallel_text in all_parallel_text:\n",
    "    parallel_tuple = (parallel_text[\"Mambai\"], parallel_text[\"English\"])\n",
    "    if parallel_tuple not in unique_parallel_text:\n",
    "        unique_parallel_text.append(parallel_tuple)\n",
    "\n",
    "print(f\"Total of {len(unique_parallel_text)} unique parallel text items\")\n",
    "\n",
    "# filter out elements that don't have a Mambai or English key\n",
    "unique_parallel_text = [\n",
    "    parallel_text\n",
    "    for parallel_text in unique_parallel_text\n",
    "    if parallel_text[0] != \"\" and parallel_text[1] != \"\"\n",
    "]\n",
    "\n",
    "print(f\"Total of {len(unique_parallel_text)} unique parallel text items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all_parallel_text to a csv\n",
    "import csv\n",
    "\n",
    "with open(\"mambai_parallel_text.csv\", \"w\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"Mambai\", \"English\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_parallel_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
